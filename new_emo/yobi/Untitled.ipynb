{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "560cf934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32c887f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c48e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import soundcard as sc\n",
    "import threading\n",
    "import queue\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "INTERVAL = 3\n",
    "BUFFER_SIZE = 4096\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "791c660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.4.5-py3-none-win_amd64.whl (195 kB)\n",
      "Collecting asyncio\n",
      "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\1yvfs93\\anaconda3\\lib\\site-packages (from sounddevice) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\1yvfs93\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Installing collected packages: sounddevice, asyncio\n",
      "Successfully installed asyncio-3.4.3 sounddevice-0.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c51dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "import whisper\n",
    "\n",
    "import asyncio\n",
    "import queue\n",
    "import sys\n",
    "\n",
    "\n",
    "# SETTINGS\n",
    "MODEL_TYPE=\"base\"\n",
    "# the model used for transcription. https://github.com/openai/whisper#available-models-and-languages\n",
    "LANGUAGE=\"Japanese\"\n",
    "# pre-set the language to avoid autodetection\n",
    "BLOCKSIZE=24678 \n",
    "# this is the base chunk size the audio is split into in samples. blocksize / 16000 = chunk length in seconds. \n",
    "SILENCE_THRESHOLD=400\n",
    "# should be set to the lowest sample amplitude that the speech in the audio material has\n",
    "SILENCE_RATIO=100\n",
    "# number of samples in one buffer that are allowed to be higher than threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e47580",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model\", default=\"medium\", help=\"Model to use\",\n",
    "                        choices=[\"tiny\", \"base\", \"small\", \"medium\", \"large\"])\n",
    "parser.add_argument(\"--non_english\", action='store_true',\n",
    "                        help=\"Don't use the english model.\")\n",
    "parser.add_argument(\"--energy_threshold\", default=1000,\n",
    "                        help=\"Energy level for mic to detect.\", type=int)\n",
    "parser.add_argument(\"--record_timeout\", default=2,\n",
    "                        help=\"How real time the recording is in seconds.\", type=float)\n",
    "parser.add_argument(\"--phrase_timeout\", default=3,\n",
    "                        help=\"How much empty space between recordings before we \"\n",
    "                             \"consider it a new line in the transcription.\", type=float)  \n",
    "    if 'linux' in platform:\n",
    "        parser.add_argument(\"--default_microphone\", default='pulse',\n",
    "                            help=\"Default microphone name for SpeechRecognition. \"\n",
    "                                 \"Run this with 'list' to view available Microphones.\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # The last time a recording was retreived from the queue.\n",
    "    phrase_time = None\n",
    "    # Current raw audio bytes.\n",
    "    last_sample = bytes()\n",
    "    # Thread safe Queue for passing data from the threaded recording callback.\n",
    "    data_queue = Queue()\n",
    "    # We use SpeechRecognizer to record our audio because it has a nice feauture where it can detect when speech ends.\n",
    "    recorder = sr.Recognizer()\n",
    "    recorder.energy_threshold = args.energy_threshold\n",
    "    # Definitely do this, dynamic energy compensation lowers the energy threshold dramtically to a point where the SpeechRecognizer never stops recording.\n",
    "    recorder.dynamic_energy_threshold = False\n",
    "    \n",
    "    # Important for linux users. \n",
    "    # Prevents permanent application hang and crash by using the wrong Microphone\n",
    "    if 'linux' in platform:\n",
    "        mic_name = args.default_microphone\n",
    "        if not mic_name or mic_name == 'list':\n",
    "            print(\"Available microphone devices are: \")\n",
    "            for index, name in enumerate(sr.Microphone.list_microphone_names()):\n",
    "                print(f\"Microphone with name \\\"{name}\\\" found\")   \n",
    "            return\n",
    "        else:\n",
    "            for index, name in enumerate(sr.Microphone.list_microphone_names()):\n",
    "                if mic_name in name:\n",
    "                    source = sr.Microphone(sample_rate=16000, device_index=index)\n",
    "                    break\n",
    "    else:\n",
    "        source = sr.Microphone(sample_rate=16000)\n",
    "        \n",
    "    # Load / Download model\n",
    "    model = args.model\n",
    "    if args.model != \"large\" and not args.non_english:\n",
    "        model = model + \".en\"\n",
    "    audio_model = whisper.load_model(model)\n",
    "\n",
    "    record_timeout = args.record_timeout\n",
    "    phrase_timeout = args.phrase_timeout\n",
    "\n",
    "    temp_file = NamedTemporaryFile().name\n",
    "    transcription = ['']\n",
    "    \n",
    "    with source:\n",
    "        recorder.adjust_for_ambient_noise(source)\n",
    "\n",
    "    def record_callback(_, audio:sr.AudioData) -> None:\n",
    "        \"\"\"\n",
    "        Threaded callback function to recieve audio data when recordings finish.\n",
    "        audio: An AudioData containing the recorded bytes.\n",
    "        \"\"\"\n",
    "        # Grab the raw bytes and push it into the thread safe queue.\n",
    "        data = audio.get_raw_data()\n",
    "        data_queue.put(data)\n",
    "\n",
    "    # Create a background thread that will pass us raw audio bytes.\n",
    "    # We could do this manually but SpeechRecognizer provides a nice helper.\n",
    "    recorder.listen_in_background(source, record_callback, phrase_time_limit=record_timeout)\n",
    "\n",
    "    # Cue the user that we're ready to go.\n",
    "    print(\"Model loaded.\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            now = datetime.utcnow()\n",
    "            # Pull raw recorded audio from the queue.\n",
    "            if not data_queue.empty():\n",
    "                phrase_complete = False\n",
    "                # If enough time has passed between recordings, consider the phrase complete.\n",
    "                # Clear the current working audio buffer to start over with the new data.\n",
    "                if phrase_time and now - phrase_time > timedelta(seconds=phrase_timeout):\n",
    "                    last_sample = bytes()\n",
    "                    phrase_complete = True\n",
    "                # This is the last time we received new audio data from the queue.\n",
    "                phrase_time = now\n",
    "\n",
    "                # Concatenate our current audio data with the latest audio data.\n",
    "                while not data_queue.empty():\n",
    "                    data = data_queue.get()\n",
    "                    last_sample += data\n",
    "\n",
    "                # Use AudioData to convert the raw data to wav data.\n",
    "                audio_data = sr.AudioData(last_sample, source.SAMPLE_RATE, source.SAMPLE_WIDTH)\n",
    "                wav_data = io.BytesIO(audio_data.get_wav_data())\n",
    "\n",
    "                # Write wav data to the temporary file as bytes.\n",
    "                with open(temp_file, 'w+b') as f:\n",
    "                    f.write(wav_data.read())\n",
    "\n",
    "                # Read the transcription.\n",
    "                result = audio_model.transcribe(temp_file, fp16=torch.cuda.is_available())\n",
    "                text = result['text'].strip()\n",
    "\n",
    "                # If we detected a pause between recordings, add a new item to our transcripion.\n",
    "                # Otherwise edit the existing one.\n",
    "                if phrase_complete:\n",
    "                    transcription.append(text)\n",
    "                else:\n",
    "                    transcription[-1] = text\n",
    "\n",
    "                # Clear the console to reprint the updated transcription.\n",
    "                os.system('cls' if os.name=='nt' else 'clear')\n",
    "                for line in transcription:\n",
    "                    print(line)\n",
    "                # Flush stdout.\n",
    "                print('', end='', flush=True)\n",
    "\n",
    "                # Infinite loops are bad for processors, must sleep.\n",
    "                sleep(0.25)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    print(\"\\n\\nTranscription:\")\n",
    "    for line in transcription:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91cd04b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     61\u001b[0m \t\u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m \t\t\u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \t\u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m \t\tsys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "global_ndarray = None\n",
    "model = whisper.load_model(MODEL_TYPE)\n",
    "\n",
    "async def inputstream_generator():\n",
    "\t\"\"\"Generator that yields blocks of input data as NumPy arrays.\"\"\"\n",
    "\tq_in = asyncio.Queue()\n",
    "\tloop = asyncio.get_event_loop()\n",
    "\n",
    "\tdef callback(indata, frame_count, time_info, status):\n",
    "\t\tloop.call_soon_threadsafe(q_in.put_nowait, (indata.copy(), status))\n",
    "\n",
    "\tstream = sd.InputStream(samplerate=16000, channels=1, dtype='int16', blocksize=BLOCKSIZE, callback=callback)\n",
    "\twith stream:\n",
    "\t\twhile True:\n",
    "\t\t\tindata, status = await q_in.get()\n",
    "\t\t\tyield indata, status\n",
    "\t\t\t\n",
    "\t\t\n",
    "async def process_audio_buffer():\n",
    "\tglobal global_ndarray\n",
    "\tasync for indata, status in inputstream_generator():\n",
    "\t\t\n",
    "\t\tindata_flattened = abs(indata.flatten())\n",
    "\t\t\t\t\n",
    "\t\t# discard buffers that contain mostly silence\n",
    "\t\tif(np.asarray(np.where(indata_flattened > SILENCE_THRESHOLD)).size < SILENCE_RATIO):\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tif (global_ndarray is not None):\n",
    "\t\t\tglobal_ndarray = np.concatenate((global_ndarray, indata), dtype='int16')\n",
    "\t\telse:\n",
    "\t\t\tglobal_ndarray = indata\n",
    "\t\t\t\n",
    "\t\t# concatenate buffers if the end of the current buffer is not silent\n",
    "\t\tif (np.average((indata_flattened[-100:-1])) > SILENCE_THRESHOLD/15):\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tlocal_ndarray = global_ndarray.copy()\n",
    "\t\t\tglobal_ndarray = None\n",
    "\t\t\tindata_transformed = local_ndarray.flatten().astype(np.float32) / 32768.0\n",
    "\t\t\tresult = model.transcribe(indata_transformed, language=LANGUAGE)\n",
    "\t\t\tprint(result[\"text\"])\n",
    "\t\t\t\n",
    "\t\tdel local_ndarray\n",
    "\t\tdel indata_flattened\n",
    "\n",
    "\n",
    "async def main():\n",
    "\tprint('\\nActivating wire ...\\n')\n",
    "\taudio_task = asyncio.create_task(process_audio_buffer())\n",
    "\twhile True:\n",
    "\t\tawait asyncio.sleep(1)\n",
    "\taudio_task.cancel()\n",
    "\ttry:\n",
    "\t\tawait audio_task\n",
    "\texcept asyncio.CancelledError:\n",
    "\t\tprint('\\nwire was cancelled')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\ttry:\n",
    "\t\tasyncio.run(main())\n",
    "\texcept KeyboardInterrupt:\n",
    "\t\tsys.exit('\\nInterrupted by user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0515e163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Loading model...')\n",
    "model = whisper.load_model(\"large\")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25276026",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = queue.Queue()\n",
    "b = np.ones(100) / 100\n",
    "\n",
    "options = whisper.DecodingOptions()\n",
    "\n",
    "def recognize():\n",
    "    while True:\n",
    "        audio = q.get()\n",
    "        if (audio ** 2).max() > 0.001:\n",
    "            audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "            # make log-Mel spectrogram and move to the same device as the model\n",
    "            mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "            # detect the spoken language\n",
    "            _, probs = model.detect_language(mel)\n",
    "\n",
    "            # decode the audio\n",
    "            result = whisper.decode(model, mel, options)\n",
    "\n",
    "            # print the recognized text\n",
    "            print(f'{max(probs, key=probs.get)}: {result.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc31ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import io\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from queue import Queue\n",
    "from tempfile import NamedTemporaryFile\n",
    "from time import sleep\n",
    "from sys import platform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a366e57c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m<\u001b[39m SAMPLE_RATE \u001b[38;5;241m*\u001b[39m INTERVAL:\n\u001b[1;32m----> 6\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mmic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBUFFER_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         audio[n:n\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m         n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\soundcard\\mediafoundation.py:767\u001b[0m, in \u001b[0;36m_Recorder.record\u001b[1;34m(self, numframes)\u001b[0m\n\u001b[0;32m    765\u001b[0m required_frames \u001b[38;5;241m=\u001b[39m numframes\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannelmap))\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m recorded_frames \u001b[38;5;241m<\u001b[39m required_frames:\n\u001b[1;32m--> 767\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    769\u001b[0m         \u001b[38;5;66;03m# no data forthcoming: return zeros\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mzeros(required_frames\u001b[38;5;241m-\u001b[39mrecorded_frames,\n\u001b[0;32m    771\u001b[0m                             dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\soundcard\\mediafoundation.py:726\u001b[0m, in \u001b[0;36m_Recorder._record_chunk\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m empty_frames \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m    724\u001b[0m         \u001b[38;5;66;03m# no data for 10 ms: give up.\u001b[39;00m\n\u001b[0;32m    725\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 726\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    727\u001b[0m data_ptr, nframes, flags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_capture_buffer()\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_ptr \u001b[38;5;241m!=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with sc.get_microphone(id=str(sc.default_speaker().name), include_loopback=True).recorder(samplerate=SAMPLE_RATE, channels=1) as mic:\n",
    "    audio = np.empty(SAMPLE_RATE * INTERVAL + BUFFER_SIZE, dtype=np.float32)\n",
    "    n = 0\n",
    "    while True:\n",
    "        while n < SAMPLE_RATE * INTERVAL:\n",
    "            data = mic.record(BUFFER_SIZE)\n",
    "            audio[n:n+len(data)] = data.reshape(-1)\n",
    "            n += len(data)\n",
    "\n",
    "        # find silent periods\n",
    "        m = n * 4 // 5\n",
    "        vol = np.convolve(audio[m:n] ** 2, b, 'same')\n",
    "        m += vol.argmin()\n",
    "        q.put(audio[:m])\n",
    "\n",
    "        audio_prev = audio\n",
    "        audio = np.empty(SAMPLE_RATE * INTERVAL + BUFFER_SIZE, dtype=np.float32)\n",
    "        audio[:n-m] = audio_prev[m:n]\n",
    "        n = n-m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aceccb",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
